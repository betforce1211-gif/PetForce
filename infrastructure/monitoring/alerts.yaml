# PetForce Monitoring & Alerting Configuration
#
# This file defines comprehensive monitoring and alerting for PetForce infrastructure.
#
# Coverage:
# - Application performance (error rate, latency, throughput)
# - Database health (connections, query performance, replication)
# - Redis cache (hit rate, memory, evictions)
# - Infrastructure (CPU, memory, disk, network)
# - Security (authentication failures, rate limiting)
# - Business metrics (household creation, API usage)
# - Cost monitoring
#
# Platforms:
# - DataDog (primary APM and infrastructure monitoring)
# - AWS CloudWatch (AWS resource monitoring)
# - Prometheus (Kubernetes metrics)
#
# Notification Channels:
# - PagerDuty (P0/P1 critical alerts)
# - Slack (all alerts)
# - Email (daily summaries)

---
# DataDog Monitors
# Deploy with: datadog-cli monitor apply -f alerts.yaml

datadog_monitors:
  # Application Performance Monitors

  - name: "[P0] High Error Rate - Household API"
    type: "metric alert"
    query: "avg(last_5m):sum:household.api.errors{env:production} by {endpoint}.as_rate() > 10"
    message: |
      **High error rate detected in Household API**

      Error rate: {{value}} errors/sec
      Endpoint: {{endpoint.name}}

      **Impact**: Users cannot create/join households
      **Action Required**: Check application logs and database connectivity

      Runbook: https://docs.petforce.app/runbooks/high-error-rate

      @pagerduty-critical @slack-alerts
    tags:
      - "service:household-api"
      - "severity:critical"
      - "team:platform"
    priority: 1
    thresholds:
      critical: 10
      warning: 5
    notify_no_data: true
    no_data_timeframe: 10

  - name: "[P1] High API Latency - Household Operations"
    type: "metric alert"
    query: "avg(last_5m):avg:household.api.latency{env:production} by {endpoint} > 2000"
    message: |
      **High latency detected in Household API**

      Latency: {{value}}ms (p95)
      Endpoint: {{endpoint.name}}

      **Impact**: Slow user experience
      **Action Required**: Check database query performance and connection pool

      Runbook: https://docs.petforce.app/runbooks/high-latency

      @pagerduty-high @slack-alerts
    tags:
      - "service:household-api"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 2000
      warning: 1000

  - name: "[P1] High Memory Usage - Web Application"
    type: "metric alert"
    query: "avg(last_10m):avg:system.mem.used{env:production,service:petforce-web} / avg:system.mem.total{env:production,service:petforce-web} * 100 > 85"
    message: |
      **High memory usage detected**

      Memory usage: {{value}}%
      Service: petforce-web

      **Impact**: Risk of OOM kills and service degradation
      **Action Required**: Check for memory leaks and scale if needed

      Runbook: https://docs.petforce.app/runbooks/high-memory

      @pagerduty-high @slack-alerts
    tags:
      - "service:petforce-web"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 85
      warning: 75

  - name: "[P2] High CPU Usage - Web Application"
    type: "metric alert"
    query: "avg(last_10m):avg:system.cpu.user{env:production,service:petforce-web} > 80"
    message: |
      **High CPU usage detected**

      CPU usage: {{value}}%
      Service: petforce-web

      **Impact**: Slow response times
      **Action Required**: Check for CPU-intensive operations and scale if needed

      @slack-alerts
    tags:
      - "service:petforce-web"
      - "severity:medium"
      - "team:platform"
    priority: 3
    thresholds:
      critical: 80
      warning: 65

  # Database Monitors

  - name: "[P0] Database Connection Pool Exhausted"
    type: "metric alert"
    query: "avg(last_5m):avg:supabase.pool.available_connections{env:production} < 5"
    message: |
      **Database connection pool nearly exhausted**

      Available connections: {{value}}

      **Impact**: API requests will fail or timeout
      **Action Required**:
      1. Check for connection leaks
      2. Review slow queries
      3. Consider scaling connection pool

      Runbook: https://docs.petforce.app/runbooks/db-connection-pool

      @pagerduty-critical @slack-alerts
    tags:
      - "service:database"
      - "severity:critical"
      - "team:platform"
    priority: 1
    thresholds:
      critical: 5
      warning: 10

  - name: "[P1] Slow Database Queries"
    type: "metric alert"
    query: "avg(last_10m):avg:supabase.query.duration{env:production} by {query_name} > 5000"
    message: |
      **Slow database queries detected**

      Query duration: {{value}}ms
      Query: {{query_name}}

      **Impact**: Slow API responses
      **Action Required**: Review query execution plan and add indexes if needed

      Runbook: https://docs.petforce.app/runbooks/slow-queries

      @pagerduty-high @slack-alerts
    tags:
      - "service:database"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 5000
      warning: 3000

  - name: "[P0] Database Connection Failures"
    type: "metric alert"
    query: "avg(last_5m):sum:supabase.connection.errors{env:production}.as_rate() > 5"
    message: |
      **Database connection failures**

      Error rate: {{value}} errors/sec

      **Impact**: Complete service outage
      **Action Required**: Check database status and network connectivity

      Runbook: https://docs.petforce.app/runbooks/db-connection-failure

      @pagerduty-critical @slack-alerts
    tags:
      - "service:database"
      - "severity:critical"
      - "team:platform"
    priority: 1
    thresholds:
      critical: 5
      warning: 2

  # Redis Cache Monitors

  - name: "[P1] Low Redis Cache Hit Rate"
    type: "metric alert"
    query: "avg(last_15m):(sum:redis.stats.keyspace_hits{env:production} / (sum:redis.stats.keyspace_hits{env:production} + sum:redis.stats.keyspace_misses{env:production})) * 100 < 70"
    message: |
      **Low Redis cache hit rate**

      Hit rate: {{value}}%

      **Impact**: Increased database load and slower responses
      **Action Required**: Review cache TTLs and cache warming strategy

      @slack-alerts
    tags:
      - "service:redis"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 70
      warning: 80

  - name: "[P1] High Redis Memory Usage"
    type: "metric alert"
    query: "avg(last_10m):avg:redis.mem.used{env:production} / avg:redis.mem.maxmemory{env:production} * 100 > 85"
    message: |
      **High Redis memory usage**

      Memory usage: {{value}}%

      **Impact**: Risk of cache evictions and performance degradation
      **Action Required**: Review cache size limits and eviction policy

      @pagerduty-high @slack-alerts
    tags:
      - "service:redis"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 85
      warning: 75

  - name: "[P2] High Redis Eviction Rate"
    type: "metric alert"
    query: "avg(last_10m):sum:redis.keys.evicted{env:production}.as_rate() > 100"
    message: |
      **High Redis eviction rate**

      Eviction rate: {{value}} keys/sec

      **Impact**: Reduced cache effectiveness
      **Action Required**: Consider increasing Redis memory or optimizing cache usage

      @slack-alerts
    tags:
      - "service:redis"
      - "severity:medium"
      - "team:platform"
    priority: 3
    thresholds:
      critical: 100
      warning: 50

  # Business Metrics Monitors

  - name: "[P1] Household Creation Failures"
    type: "metric alert"
    query: "avg(last_15m):sum:household.creation.failures{env:production}.as_rate() > 5"
    message: |
      **High household creation failure rate**

      Failure rate: {{value}} failures/min

      **Impact**: Users cannot create households
      **Action Required**: Check application logs and database constraints

      @pagerduty-high @slack-alerts
    tags:
      - "service:household-api"
      - "severity:high"
      - "team:product"
    priority: 2
    thresholds:
      critical: 5
      warning: 2

  - name: "[P2] Low Household Join Success Rate"
    type: "metric alert"
    query: "avg(last_30m):(sum:household.join.success{env:production} / (sum:household.join.success{env:production} + sum:household.join.failures{env:production})) * 100 < 85"
    message: |
      **Low household join success rate**

      Success rate: {{value}}%

      **Impact**: Users having trouble joining households
      **Action Required**: Review join flow and invite code validation

      @slack-alerts
    tags:
      - "service:household-api"
      - "severity:medium"
      - "team:product"
    priority: 3
    thresholds:
      critical: 85
      warning: 90

  # Security Monitors

  - name: "[P0] High Authentication Failure Rate"
    type: "metric alert"
    query: "avg(last_5m):sum:auth.login.failures{env:production}.as_rate() > 20"
    message: |
      **High authentication failure rate**

      Failure rate: {{value}} failures/sec

      **Impact**: Possible credential stuffing attack
      **Action Required**: Review authentication logs and consider rate limiting

      Runbook: https://docs.petforce.app/runbooks/auth-failures

      @pagerduty-critical @slack-alerts
    tags:
      - "service:auth"
      - "severity:critical"
      - "team:security"
    priority: 1
    thresholds:
      critical: 20
      warning: 10

  - name: "[P1] Rate Limit Exceeded"
    type: "metric alert"
    query: "avg(last_10m):sum:rate_limit.exceeded{env:production} by {ip_address}.as_rate() > 100"
    message: |
      **High rate limit exceeded count**

      IP: {{ip_address}}
      Rate: {{value}} requests/sec

      **Impact**: Possible abuse or DoS attempt
      **Action Required**: Review IP and consider blocking if malicious

      @slack-alerts
    tags:
      - "service:api"
      - "severity:high"
      - "team:security"
    priority: 2
    thresholds:
      critical: 100
      warning: 50

  # Infrastructure Monitors

  - name: "[P0] Service Health Check Failing"
    type: "service check"
    query: '"http.can_connect".over("env:production","service:petforce-web").by("instance").last(4).count_by_status()'
    message: |
      **Service health check failing**

      Instance: {{instance}}

      **Impact**: Service is down or unreachable
      **Action Required**: Check service logs and container status

      Runbook: https://docs.petforce.app/runbooks/service-down

      @pagerduty-critical @slack-alerts
    tags:
      - "service:petforce-web"
      - "severity:critical"
      - "team:platform"
    priority: 1
    thresholds:
      critical: 3
      warning: 2

  - name: "[P1] High Disk Usage"
    type: "metric alert"
    query: "avg(last_10m):avg:system.disk.used{env:production,device:/} / avg:system.disk.total{env:production,device:/} * 100 > 85"
    message: |
      **High disk usage**

      Disk usage: {{value}}%
      Device: /

      **Impact**: Risk of disk full and service failures
      **Action Required**: Clean up logs and temporary files, or expand disk

      @pagerduty-high @slack-alerts
    tags:
      - "service:infrastructure"
      - "severity:high"
      - "team:platform"
    priority: 2
    thresholds:
      critical: 85
      warning: 75

  - name: "[P2] SSL Certificate Expiring Soon"
    type: "metric alert"
    query: "avg(last_1h):min:http.ssl.days_left{env:production} < 14"
    message: |
      **SSL certificate expiring soon**

      Days left: {{value}}

      **Impact**: Service will become inaccessible when cert expires
      **Action Required**: Renew SSL certificate

      @slack-alerts
    tags:
      - "service:infrastructure"
      - "severity:medium"
      - "team:platform"
    priority: 3
    thresholds:
      critical: 7
      warning: 14

  # Cost Monitoring

  - name: "[P2] High AWS Cost Increase"
    type: "metric alert"
    query: "avg(last_1d):avg:aws.estimated.charges{env:production} - avg:aws.estimated.charges{env:production}.rollup(avg, 604800) > 50"
    message: |
      **Unexpected AWS cost increase**

      Cost increase: ${{value}} vs last week

      **Action Required**: Review AWS Cost Explorer for anomalies

      @slack-alerts
    tags:
      - "service:infrastructure"
      - "severity:medium"
      - "team:platform"
    priority: 3
    thresholds:
      critical: 100
      warning: 50

---
# AWS CloudWatch Alarms
# Deploy with: aws cloudwatch put-metric-alarm --cli-input-json file://alarm.json

cloudwatch_alarms:
  - AlarmName: "PetForce-ECS-HighCPU"
    AlarmDescription: "ECS service CPU utilization is too high"
    MetricName: "CPUUtilization"
    Namespace: "AWS/ECS"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 80.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "ServiceName"
        Value: "petforce-web-service"
      - Name: "ClusterName"
        Value: "petforce-production"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"
    TreatMissingData: "notBreaching"

  - AlarmName: "PetForce-ECS-HighMemory"
    AlarmDescription: "ECS service memory utilization is too high"
    MetricName: "MemoryUtilization"
    Namespace: "AWS/ECS"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 85.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "ServiceName"
        Value: "petforce-web-service"
      - Name: "ClusterName"
        Value: "petforce-production"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"

  - AlarmName: "PetForce-ALB-HighLatency"
    AlarmDescription: "Application Load Balancer target response time is high"
    MetricName: "TargetResponseTime"
    Namespace: "AWS/ApplicationELB"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 2.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "LoadBalancer"
        Value: "app/petforce-alb/xyz123"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"

  - AlarmName: "PetForce-ALB-High5xxErrors"
    AlarmDescription: "Application Load Balancer has high 5xx error rate"
    MetricName: "HTTPCode_Target_5XX_Count"
    Namespace: "AWS/ApplicationELB"
    Statistic: "Sum"
    Period: 300
    EvaluationPeriods: 1
    Threshold: 10.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "LoadBalancer"
        Value: "app/petforce-alb/xyz123"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-critical"
    TreatMissingData: "notBreaching"

  - AlarmName: "PetForce-RDS-HighCPU"
    AlarmDescription: "RDS instance CPU is too high"
    MetricName: "CPUUtilization"
    Namespace: "AWS/RDS"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 80.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "DBInstanceIdentifier"
        Value: "petforce-postgres-prod"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"

  - AlarmName: "PetForce-RDS-LowFreeStorage"
    AlarmDescription: "RDS instance is running out of storage"
    MetricName: "FreeStorageSpace"
    Namespace: "AWS/RDS"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 1
    Threshold: 10737418240.0 # 10 GB in bytes
    ComparisonOperator: "LessThanThreshold"
    Dimensions:
      - Name: "DBInstanceIdentifier"
        Value: "petforce-postgres-prod"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-critical"

  - AlarmName: "PetForce-ElastiCache-HighCPU"
    AlarmDescription: "ElastiCache Redis CPU is too high"
    MetricName: "CPUUtilization"
    Namespace: "AWS/ElastiCache"
    Statistic: "Average"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 75.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "CacheClusterId"
        Value: "petforce-redis-prod"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"

  - AlarmName: "PetForce-ElastiCache-HighEvictions"
    AlarmDescription: "ElastiCache Redis has high eviction rate"
    MetricName: "Evictions"
    Namespace: "AWS/ElastiCache"
    Statistic: "Sum"
    Period: 300
    EvaluationPeriods: 2
    Threshold: 1000.0
    ComparisonOperator: "GreaterThanThreshold"
    Dimensions:
      - Name: "CacheClusterId"
        Value: "petforce-redis-prod"
    AlarmActions:
      - "arn:aws:sns:us-east-1:ACCOUNT_ID:petforce-alerts"

---
# Prometheus Alert Rules
# For Kubernetes deployments with Prometheus Operator

prometheus_rules:
  - name: petforce-alerts
    interval: 30s
    rules:
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{namespace="petforce"}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value }} times in the last 5 minutes"
          runbook_url: "https://docs.petforce.app/runbooks/pod-crash-looping"

      - alert: PodNotReady
        expr: sum by (namespace, pod) (kube_pod_status_phase{namespace="petforce",phase!~"Running|Succeeded"}) > 0
        for: 10m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Pod {{ $labels.pod }} not ready"
          description: "Pod {{ $labels.pod }} has been in a non-ready state for more than 10 minutes"

      - alert: DeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas{namespace="petforce"} != kube_deployment_status_replicas_available{namespace="petforce"}
        for: 10m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Deployment {{ $labels.deployment }} replica mismatch"
          description: "Deployment {{ $labels.deployment }} has {{ $value }} replicas but should have {{ $labels.spec_replicas }}"

      - alert: HighPodMemoryUsage
        expr: (sum(container_memory_working_set_bytes{namespace="petforce"}) by (pod) / sum(container_spec_memory_limit_bytes{namespace="petforce"}) by (pod) * 100) > 85
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Pod {{ $labels.pod }} high memory usage"
          description: "Pod {{ $labels.pod }} is using {{ $value }}% of its memory limit"

      - alert: HighPodCPUUsage
        expr: (sum(rate(container_cpu_usage_seconds_total{namespace="petforce"}[5m])) by (pod) / sum(container_spec_cpu_quota{namespace="petforce"}/container_spec_cpu_period{namespace="petforce"}) by (pod) * 100) > 85
        for: 10m
        labels:
          severity: medium
          team: platform
        annotations:
          summary: "Pod {{ $labels.pod }} high CPU usage"
          description: "Pod {{ $labels.pod }} is using {{ $value }}% of its CPU limit"

      - alert: PersistentVolumeUsage
        expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} high usage"
          description: "PersistentVolume {{ $labels.persistentvolumeclaim }} is {{ $value }}% full"

      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Node {{ $labels.node }} not ready"
          description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes"

      - alert: NodeMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Node {{ $labels.node }} under memory pressure"
          description: "Node {{ $labels.node }} is experiencing memory pressure"

      - alert: NodeDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: high
          team: platform
        annotations:
          summary: "Node {{ $labels.node }} under disk pressure"
          description: "Node {{ $labels.node }} is experiencing disk pressure"

---
# Notification Channels Configuration

notification_channels:
  pagerduty:
    critical:
      integration_key: "{{ PAGERDUTY_CRITICAL_KEY }}"
      urgency: high
      escalation_policy: "Platform On-Call"

    high:
      integration_key: "{{ PAGERDUTY_HIGH_KEY }}"
      urgency: low
      escalation_policy: "Platform On-Call"

  slack:
    alerts:
      webhook_url: "{{ SLACK_ALERTS_WEBHOOK }}"
      channel: "#petforce-alerts"
      username: "PetForce Monitoring"
      icon_emoji: ":rotating_light:"

    critical:
      webhook_url: "{{ SLACK_CRITICAL_WEBHOOK }}"
      channel: "#petforce-critical"
      username: "PetForce Critical"
      icon_emoji: ":fire:"
      mention: "@channel"

  email:
    daily_summary:
      recipients:
        - "platform-team@petforce.app"
      schedule: "0 9 * * *" # 9 AM daily
      include:
        - alert_summary
        - performance_metrics
        - cost_summary

---
# Alert Escalation Policies

escalation_policies:
  - name: "P0 - Critical"
    description: "Service outage or data loss"
    steps:
      - notify: ["pagerduty-critical", "slack-critical"]
        delay: 0
      - notify: ["platform-lead"]
        delay: 300 # 5 minutes
      - notify: ["engineering-manager"]
        delay: 600 # 10 minutes
      - notify: ["cto"]
        delay: 1800 # 30 minutes

  - name: "P1 - High"
    description: "Severe degradation"
    steps:
      - notify: ["pagerduty-high", "slack-alerts"]
        delay: 0
      - notify: ["platform-lead"]
        delay: 600 # 10 minutes

  - name: "P2 - Medium"
    description: "Moderate impact"
    steps:
      - notify: ["slack-alerts"]
        delay: 0
      - notify: ["platform-team-email"]
        delay: 1800 # 30 minutes

  - name: "P3 - Low"
    description: "Minor issues"
    steps:
      - notify: ["slack-alerts"]
        delay: 0

---
# Runbook Links

runbooks:
  high-error-rate: "https://docs.petforce.app/runbooks/high-error-rate"
  high-latency: "https://docs.petforce.app/runbooks/high-latency"
  high-memory: "https://docs.petforce.app/runbooks/high-memory"
  db-connection-pool: "https://docs.petforce.app/runbooks/db-connection-pool"
  slow-queries: "https://docs.petforce.app/runbooks/slow-queries"
  db-connection-failure: "https://docs.petforce.app/runbooks/db-connection-failure"
  auth-failures: "https://docs.petforce.app/runbooks/auth-failures"
  service-down: "https://docs.petforce.app/runbooks/service-down"
  pod-crash-looping: "https://docs.petforce.app/runbooks/pod-crash-looping"

---
# Alert Testing

# Test alerts with:
# datadog-cli monitor mute <monitor-id>
# datadog-cli monitor test <monitor-id>
#
# For Prometheus:
# kubectl -n monitoring exec -it prometheus-0 -- promtool test rules alerts.yaml
#
# For CloudWatch:
# aws cloudwatch set-alarm-state --alarm-name "PetForce-ECS-HighCPU" --state-value ALARM --state-reason "Testing alert"
