# .buck.yml - Buck Data Engineering Configuration

version: 1

# ==============================================
# PROJECT
# ==============================================
project:
  name: "{{PROJECT_NAME}}"
  organization: "{{ORGANIZATION}}"

# ==============================================
# DATA WAREHOUSE
# ==============================================
warehouse:
  type: snowflake  # snowflake | bigquery | redshift | databricks
  
  # Snowflake configuration
  snowflake:
    account: "{{SNOWFLAKE_ACCOUNT}}"
    database: "analytics"
    warehouse: "transforming_wh"
    role: "transformer"
    
  # BigQuery configuration (alternative)
  bigquery:
    project: "{{GCP_PROJECT}}"
    dataset: "analytics"
    location: "US"
    
  # Schema organization
  schemas:
    raw: "raw"           # Bronze layer
    staging: "staging"   # Silver layer
    intermediate: "intermediate"
    marts: "marts"       # Gold layer

# ==============================================
# DBT CONFIGURATION
# ==============================================
dbt:
  version: "1.7"
  project_name: "analytics"
  profile: "analytics"
  
  # Model configurations by folder
  models:
    staging:
      materialized: view
      schema: staging
      tags: ["staging"]
      
    intermediate:
      materialized: ephemeral
      
    marts:
      core:
        materialized: table
        schema: marts
      finance:
        materialized: incremental
        schema: finance
      marketing:
        materialized: table
        schema: marketing
        
  # Default test configurations
  tests:
    severity: error
    
  # Variables
  vars:
    start_date: "2020-01-01"
    timezone: "UTC"

# ==============================================
# DATA SOURCES
# ==============================================
sources:
  # Application database
  postgres:
    type: database
    connector: airbyte
    connection_id: "postgres-source"
    host: "{{POSTGRES_HOST}}"
    database: "production"
    schema: "public"
    tables:
      - name: users
        sync_mode: incremental
        cursor_field: updated_at
      - name: orders
        sync_mode: incremental
        cursor_field: created_at
      - name: products
        sync_mode: full_refresh
        
  # Payment provider
  stripe:
    type: api
    connector: airbyte
    connection_id: "stripe-source"
    sync_frequency: hourly
    streams:
      - charges
      - customers
      - subscriptions
      - invoices
      
  # CRM
  hubspot:
    type: api
    connector: airbyte
    connection_id: "hubspot-source"
    sync_frequency: daily
    streams:
      - contacts
      - companies
      - deals
      
  # Analytics events
  segment:
    type: api
    connector: airbyte
    connection_id: "segment-source"
    sync_frequency: hourly
    streams:
      - tracks
      - identifies
      - pages

# ==============================================
# ORCHESTRATION
# ==============================================
orchestration:
  engine: airflow  # airflow | dagster | prefect
  
  # DAG configurations
  dags:
    # Main ELT pipeline
    daily_elt:
      schedule: "0 6 * * *"  # 6 AM UTC daily
      catchup: false
      max_active_runs: 1
      sla_deadline: "08:00"
      tasks:
        - extract_sources
        - dbt_staging
        - dbt_intermediate
        - dbt_marts
        - dbt_test
        - notify
        
    # Hourly metrics refresh
    hourly_metrics:
      schedule: "0 * * * *"
      catchup: false
      tasks:
        - extract_stripe
        - dbt_run_tagged:hourly
        - dbt_test_tagged:hourly
        
    # Weekly full refresh
    weekly_full_refresh:
      schedule: "0 2 * * 0"  # 2 AM Sunday
      tasks:
        - dbt_run_full_refresh
        - dbt_test
        - vacuum_tables

# ==============================================
# DATA QUALITY
# ==============================================
quality:
  # Test framework
  framework: dbt_tests  # dbt_tests | great_expectations | soda
  
  # Default tests for all models
  default_tests:
    - not_null:
        columns: [primary_key]
    - unique:
        columns: [primary_key]
        
  # Freshness checks
  freshness:
    default:
      warn_after:
        count: 12
        period: hour
      error_after:
        count: 24
        period: hour
        
  # Row count anomaly detection
  anomaly_detection:
    enabled: true
    threshold_percent: 30  # Alert if >30% change
    
  # Custom tests
  custom_tests:
    - name: assert_positive_revenue
      model: fct_orders
      test: "revenue >= 0"
    - name: assert_valid_email
      model: dim_customers
      test: "email LIKE '%@%.%'"

# ==============================================
# PERFORMANCE
# ==============================================
performance:
  # Incremental configurations
  incremental:
    default_strategy: merge
    lookback_days: 3
    
  # Partitioning
  partitioning:
    default_field: created_at
    default_granularity: day
    
  # Clustering
  clustering:
    fct_orders:
      keys: [customer_id, ordered_at]
    fct_events:
      keys: [user_id, event_type, timestamp]
      
  # Warehouse sizing
  warehouse_sizes:
    extract: X-Small
    transform: Medium
    heavy_transform: Large

# ==============================================
# MONITORING
# ==============================================
monitoring:
  # Metrics to track
  metrics:
    - pipeline_duration
    - model_run_time
    - row_counts
    - test_pass_rate
    - freshness_age
    - warehouse_cost
    
  # Alerting
  alerts:
    channels:
      slack:
        webhook: "{{SLACK_DATA_WEBHOOK}}"
        channel: "#data-alerts"
      pagerduty:
        service_key: "{{PAGERDUTY_KEY}}"
        
    rules:
      - name: Pipeline Failed
        condition: "status == 'failed'"
        severity: critical
        channels: [slack, pagerduty]
        
      - name: Data Stale
        condition: "freshness_hours > 24"
        severity: high
        channels: [slack]
        
      - name: Row Count Anomaly
        condition: "row_count_change_pct > 30"
        severity: medium
        channels: [slack]

# ==============================================
# COST MANAGEMENT
# ==============================================
cost:
  # Warehouse auto-suspend
  auto_suspend_seconds: 300
  
  # Query timeout
  query_timeout_seconds: 3600
  
  # Budget alerts
  budget:
    monthly_limit: 5000
    alert_thresholds: [50, 80, 100]
    
  # Optimization
  optimization:
    - use_incremental_where_possible
    - avoid_select_star
    - partition_large_tables
    - cluster_frequently_joined_columns

# ==============================================
# DOCUMENTATION
# ==============================================
documentation:
  # Auto-generate docs
  dbt_docs:
    enabled: true
    serve_port: 8080
    
  # Data catalog
  catalog:
    type: datahub  # datahub | amundsen | none
    
  # Column descriptions required
  require_descriptions: true
  
  # Lineage tracking
  lineage:
    enabled: true
    depth: 5
